{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Twitter Streaming APP\n",
    "In order to use all of this though, we need to setup a Developer API acocunt with Twitter and create an application to get credentials. Review the video for instructions on how to do this or if you are already familiar with it, just get the credentials from: \n",
    "\n",
    "    https://apps.twitter.com/\n",
    "    \n",
    "Once you have that you also need to install python-twitter, a python library to connect your Python to the twitter dev account.\n",
    "\n",
    "You probably won't be able to run this example and then previous in the same notebook, you need to restart you kernel.\n",
    "\n",
    "Let's get started!\n",
    "\n",
    "Begin by running the TweetRead.py file. Make sure to add your own IP Adress and your credential keys."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# May cause deprecation warnings, safe to ignore, they aren't errors\n",
    "from pyspark import SparkContext\n",
    "from pyspark.streaming import StreamingContext\n",
    "from pyspark.sql import SQLContext\n",
    "from pyspark.sql.functions import desc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "23/08/23 21:03:06 WARN Utils: Your hostname, Blade-15-Base-Model resolves to a loopback address: 127.0.1.1; using 192.168.0.23 instead (on interface wlo1)\n",
      "23/08/23 21:03:06 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "23/08/23 21:03:08 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    }
   ],
   "source": [
    "# Can only run this once. restart your kernel for any errors.\n",
    "sc = SparkContext()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ilyes/.local/lib/python3.10/site-packages/pyspark/streaming/context.py:72: FutureWarning: DStream is deprecated as of Spark 3.4.0. Migrate to Structured Streaming.\n",
      "  warnings.warn(\n",
      "/home/ilyes/.local/lib/python3.10/site-packages/pyspark/sql/context.py:112: FutureWarning: Deprecated in 3.0.0. Use SparkSession.builder.getOrCreate() instead.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "ssc = StreamingContext(sc, 10 )\n",
    "sqlContext = SQLContext(sc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "socket_stream = ssc.socketTextStream(\"127.0.0.1\", 9998)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "lines = socket_stream.window( 20 )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from collections import namedtuple\n",
    "fields = (\"tag\", \"count\" )\n",
    "Tweet = namedtuple( 'Tweet', fields )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Use Parenthesis for multiple lines or use \\.\n",
    "( lines.flatMap( lambda text: text.split( \" \" ) ) #Splits to a list\n",
    "  .filter( lambda word: word.lower().startswith(\"#\") ) # Checks for hashtag calls\n",
    "  .map( lambda word: ( word.lower(), 1 ) ) # Lower cases the word\n",
    "  .reduceByKey( lambda a, b: a + b ) # Reduces\n",
    "  .map( lambda rec: Tweet( rec[0], rec[1] ) ) # Stores in a Tweet Object\n",
    "  .foreachRDD( lambda rdd: rdd.toDF().sort( desc(\"count\") ) # Sorts Them in a DF\n",
    "  .limit(10).registerTempTable(\"tweets\") ) ) # Registers to a table."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__________\n",
    "### Now run TweetRead.py\n",
    "__________"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "23/08/23 21:03:43 ERROR JobScheduler: Error running job streaming job 1692817420000 ms.0\n",
      "org.apache.spark.SparkException: An exception was raised by Python:\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/ilyes/.local/lib/python3.10/site-packages/pyspark/streaming/util.py\", line 71, in call\n",
      "    r = self.func(t, *rdds)\n",
      "  File \"/home/ilyes/.local/lib/python3.10/site-packages/pyspark/streaming/dstream.py\", line 236, in func\n",
      "    return old_func(rdd)  # type: ignore[call-arg, arg-type]\n",
      "  File \"/tmp/ipykernel_4072/1757346720.py\", line 7, in <lambda>\n",
      "    .foreachRDD( lambda rdd: rdd.toDF().sort( desc(\"count\") ) # Sorts Them in a DF\n",
      "  File \"/home/ilyes/.local/lib/python3.10/site-packages/pyspark/sql/session.py\", line 115, in toDF\n",
      "    return sparkSession.createDataFrame(self, schema, sampleRatio)\n",
      "  File \"/home/ilyes/.local/lib/python3.10/site-packages/pyspark/sql/session.py\", line 1276, in createDataFrame\n",
      "    return self._create_dataframe(\n",
      "  File \"/home/ilyes/.local/lib/python3.10/site-packages/pyspark/sql/session.py\", line 1316, in _create_dataframe\n",
      "    rdd, struct = self._createFromRDD(data.map(prepare), schema, samplingRatio)\n",
      "  File \"/home/ilyes/.local/lib/python3.10/site-packages/pyspark/sql/session.py\", line 931, in _createFromRDD\n",
      "    struct = self._inferSchema(rdd, samplingRatio, names=schema)\n",
      "  File \"/home/ilyes/.local/lib/python3.10/site-packages/pyspark/sql/session.py\", line 874, in _inferSchema\n",
      "    first = rdd.first()\n",
      "  File \"/home/ilyes/.local/lib/python3.10/site-packages/pyspark/rdd.py\", line 2872, in first\n",
      "    raise ValueError(\"RDD is empty\")\n",
      "ValueError: RDD is empty\n",
      "\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.callPythonTransformFunction(PythonDStream.scala:95)\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.apply(PythonDStream.scala:78)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1$adapted(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$2(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.streaming.dstream.DStream.createRDDWithLocalProperties(DStream.scala:417)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$1(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.Try$.apply(Try.scala:213)\n",
      "\tat org.apache.spark.streaming.scheduler.Job.run(Job.scala:39)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.$anonfun$run$1(JobScheduler.scala:256)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.run(JobScheduler.scala:256)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:829)\n",
      "23/08/23 21:03:51 ERROR JobScheduler: Error running job streaming job 1692817430000 ms.0\n",
      "org.apache.spark.SparkException: An exception was raised by Python:\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/ilyes/.local/lib/python3.10/site-packages/pyspark/streaming/util.py\", line 71, in call\n",
      "    r = self.func(t, *rdds)\n",
      "  File \"/home/ilyes/.local/lib/python3.10/site-packages/pyspark/streaming/dstream.py\", line 236, in func\n",
      "    return old_func(rdd)  # type: ignore[call-arg, arg-type]\n",
      "  File \"/tmp/ipykernel_4072/1757346720.py\", line 7, in <lambda>\n",
      "    .foreachRDD( lambda rdd: rdd.toDF().sort( desc(\"count\") ) # Sorts Them in a DF\n",
      "  File \"/home/ilyes/.local/lib/python3.10/site-packages/pyspark/sql/session.py\", line 115, in toDF\n",
      "    return sparkSession.createDataFrame(self, schema, sampleRatio)\n",
      "  File \"/home/ilyes/.local/lib/python3.10/site-packages/pyspark/sql/session.py\", line 1276, in createDataFrame\n",
      "    return self._create_dataframe(\n",
      "  File \"/home/ilyes/.local/lib/python3.10/site-packages/pyspark/sql/session.py\", line 1316, in _create_dataframe\n",
      "    rdd, struct = self._createFromRDD(data.map(prepare), schema, samplingRatio)\n",
      "  File \"/home/ilyes/.local/lib/python3.10/site-packages/pyspark/sql/session.py\", line 931, in _createFromRDD\n",
      "    struct = self._inferSchema(rdd, samplingRatio, names=schema)\n",
      "  File \"/home/ilyes/.local/lib/python3.10/site-packages/pyspark/sql/session.py\", line 874, in _inferSchema\n",
      "    first = rdd.first()\n",
      "  File \"/home/ilyes/.local/lib/python3.10/site-packages/pyspark/rdd.py\", line 2872, in first\n",
      "    raise ValueError(\"RDD is empty\")\n",
      "ValueError: RDD is empty\n",
      "\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.callPythonTransformFunction(PythonDStream.scala:95)\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.apply(PythonDStream.scala:78)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1$adapted(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$2(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.streaming.dstream.DStream.createRDDWithLocalProperties(DStream.scala:417)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$1(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.Try$.apply(Try.scala:213)\n",
      "\tat org.apache.spark.streaming.scheduler.Job.run(Job.scala:39)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.$anonfun$run$1(JobScheduler.scala:256)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.run(JobScheduler.scala:256)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:829)\n",
      "23/08/23 21:04:01 ERROR JobScheduler: Error running job streaming job 1692817440000 ms.0\n",
      "org.apache.spark.SparkException: An exception was raised by Python:\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/ilyes/.local/lib/python3.10/site-packages/pyspark/streaming/util.py\", line 71, in call\n",
      "    r = self.func(t, *rdds)\n",
      "  File \"/home/ilyes/.local/lib/python3.10/site-packages/pyspark/streaming/dstream.py\", line 236, in func\n",
      "    return old_func(rdd)  # type: ignore[call-arg, arg-type]\n",
      "  File \"/tmp/ipykernel_4072/1757346720.py\", line 7, in <lambda>\n",
      "    .foreachRDD( lambda rdd: rdd.toDF().sort( desc(\"count\") ) # Sorts Them in a DF\n",
      "  File \"/home/ilyes/.local/lib/python3.10/site-packages/pyspark/sql/session.py\", line 115, in toDF\n",
      "    return sparkSession.createDataFrame(self, schema, sampleRatio)\n",
      "  File \"/home/ilyes/.local/lib/python3.10/site-packages/pyspark/sql/session.py\", line 1276, in createDataFrame\n",
      "    return self._create_dataframe(\n",
      "  File \"/home/ilyes/.local/lib/python3.10/site-packages/pyspark/sql/session.py\", line 1316, in _create_dataframe\n",
      "    rdd, struct = self._createFromRDD(data.map(prepare), schema, samplingRatio)\n",
      "  File \"/home/ilyes/.local/lib/python3.10/site-packages/pyspark/sql/session.py\", line 931, in _createFromRDD\n",
      "    struct = self._inferSchema(rdd, samplingRatio, names=schema)\n",
      "  File \"/home/ilyes/.local/lib/python3.10/site-packages/pyspark/sql/session.py\", line 874, in _inferSchema\n",
      "    first = rdd.first()\n",
      "  File \"/home/ilyes/.local/lib/python3.10/site-packages/pyspark/rdd.py\", line 2872, in first\n",
      "    raise ValueError(\"RDD is empty\")\n",
      "ValueError: RDD is empty\n",
      "\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.callPythonTransformFunction(PythonDStream.scala:95)\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.apply(PythonDStream.scala:78)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1$adapted(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$2(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.streaming.dstream.DStream.createRDDWithLocalProperties(DStream.scala:417)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$1(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.Try$.apply(Try.scala:213)\n",
      "\tat org.apache.spark.streaming.scheduler.Job.run(Job.scala:39)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.$anonfun$run$1(JobScheduler.scala:256)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.run(JobScheduler.scala:256)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:829)\n",
      "23/08/23 21:04:11 ERROR JobScheduler: Error running job streaming job 1692817450000 ms.0\n",
      "org.apache.spark.SparkException: An exception was raised by Python:\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/ilyes/.local/lib/python3.10/site-packages/pyspark/streaming/util.py\", line 71, in call\n",
      "    r = self.func(t, *rdds)\n",
      "  File \"/home/ilyes/.local/lib/python3.10/site-packages/pyspark/streaming/dstream.py\", line 236, in func\n",
      "    return old_func(rdd)  # type: ignore[call-arg, arg-type]\n",
      "  File \"/tmp/ipykernel_4072/1757346720.py\", line 7, in <lambda>\n",
      "    .foreachRDD( lambda rdd: rdd.toDF().sort( desc(\"count\") ) # Sorts Them in a DF\n",
      "  File \"/home/ilyes/.local/lib/python3.10/site-packages/pyspark/sql/session.py\", line 115, in toDF\n",
      "    return sparkSession.createDataFrame(self, schema, sampleRatio)\n",
      "  File \"/home/ilyes/.local/lib/python3.10/site-packages/pyspark/sql/session.py\", line 1276, in createDataFrame\n",
      "    return self._create_dataframe(\n",
      "  File \"/home/ilyes/.local/lib/python3.10/site-packages/pyspark/sql/session.py\", line 1316, in _create_dataframe\n",
      "    rdd, struct = self._createFromRDD(data.map(prepare), schema, samplingRatio)\n",
      "  File \"/home/ilyes/.local/lib/python3.10/site-packages/pyspark/sql/session.py\", line 931, in _createFromRDD\n",
      "    struct = self._inferSchema(rdd, samplingRatio, names=schema)\n",
      "  File \"/home/ilyes/.local/lib/python3.10/site-packages/pyspark/sql/session.py\", line 874, in _inferSchema\n",
      "    first = rdd.first()\n",
      "  File \"/home/ilyes/.local/lib/python3.10/site-packages/pyspark/rdd.py\", line 2872, in first\n",
      "    raise ValueError(\"RDD is empty\")\n",
      "ValueError: RDD is empty\n",
      "\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.callPythonTransformFunction(PythonDStream.scala:95)\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.apply(PythonDStream.scala:78)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1$adapted(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$2(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.streaming.dstream.DStream.createRDDWithLocalProperties(DStream.scala:417)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$1(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.Try$.apply(Try.scala:213)\n",
      "\tat org.apache.spark.streaming.scheduler.Job.run(Job.scala:39)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.$anonfun$run$1(JobScheduler.scala:256)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.run(JobScheduler.scala:256)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:829)\n",
      "23/08/23 21:04:21 ERROR JobScheduler: Error running job streaming job 1692817460000 ms.0\n",
      "org.apache.spark.SparkException: An exception was raised by Python:\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/ilyes/.local/lib/python3.10/site-packages/pyspark/streaming/util.py\", line 71, in call\n",
      "    r = self.func(t, *rdds)\n",
      "  File \"/home/ilyes/.local/lib/python3.10/site-packages/pyspark/streaming/dstream.py\", line 236, in func\n",
      "    return old_func(rdd)  # type: ignore[call-arg, arg-type]\n",
      "  File \"/tmp/ipykernel_4072/1757346720.py\", line 7, in <lambda>\n",
      "    .foreachRDD( lambda rdd: rdd.toDF().sort( desc(\"count\") ) # Sorts Them in a DF\n",
      "  File \"/home/ilyes/.local/lib/python3.10/site-packages/pyspark/sql/session.py\", line 115, in toDF\n",
      "    return sparkSession.createDataFrame(self, schema, sampleRatio)\n",
      "  File \"/home/ilyes/.local/lib/python3.10/site-packages/pyspark/sql/session.py\", line 1276, in createDataFrame\n",
      "    return self._create_dataframe(\n",
      "  File \"/home/ilyes/.local/lib/python3.10/site-packages/pyspark/sql/session.py\", line 1316, in _create_dataframe\n",
      "    rdd, struct = self._createFromRDD(data.map(prepare), schema, samplingRatio)\n",
      "  File \"/home/ilyes/.local/lib/python3.10/site-packages/pyspark/sql/session.py\", line 931, in _createFromRDD\n",
      "    struct = self._inferSchema(rdd, samplingRatio, names=schema)\n",
      "  File \"/home/ilyes/.local/lib/python3.10/site-packages/pyspark/sql/session.py\", line 874, in _inferSchema\n",
      "    first = rdd.first()\n",
      "  File \"/home/ilyes/.local/lib/python3.10/site-packages/pyspark/rdd.py\", line 2872, in first\n",
      "    raise ValueError(\"RDD is empty\")\n",
      "ValueError: RDD is empty\n",
      "\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.callPythonTransformFunction(PythonDStream.scala:95)\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.apply(PythonDStream.scala:78)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1$adapted(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$2(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.streaming.dstream.DStream.createRDDWithLocalProperties(DStream.scala:417)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$1(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.Try$.apply(Try.scala:213)\n",
      "\tat org.apache.spark.streaming.scheduler.Job.run(Job.scala:39)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.$anonfun$run$1(JobScheduler.scala:256)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.run(JobScheduler.scala:256)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:829)\n",
      "23/08/23 21:04:31 ERROR JobScheduler: Error running job streaming job 1692817470000 ms.0\n",
      "org.apache.spark.SparkException: An exception was raised by Python:\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/ilyes/.local/lib/python3.10/site-packages/pyspark/streaming/util.py\", line 71, in call\n",
      "    r = self.func(t, *rdds)\n",
      "  File \"/home/ilyes/.local/lib/python3.10/site-packages/pyspark/streaming/dstream.py\", line 236, in func\n",
      "    return old_func(rdd)  # type: ignore[call-arg, arg-type]\n",
      "  File \"/tmp/ipykernel_4072/1757346720.py\", line 7, in <lambda>\n",
      "    .foreachRDD( lambda rdd: rdd.toDF().sort( desc(\"count\") ) # Sorts Them in a DF\n",
      "  File \"/home/ilyes/.local/lib/python3.10/site-packages/pyspark/sql/session.py\", line 115, in toDF\n",
      "    return sparkSession.createDataFrame(self, schema, sampleRatio)\n",
      "  File \"/home/ilyes/.local/lib/python3.10/site-packages/pyspark/sql/session.py\", line 1276, in createDataFrame\n",
      "    return self._create_dataframe(\n",
      "  File \"/home/ilyes/.local/lib/python3.10/site-packages/pyspark/sql/session.py\", line 1316, in _create_dataframe\n",
      "    rdd, struct = self._createFromRDD(data.map(prepare), schema, samplingRatio)\n",
      "  File \"/home/ilyes/.local/lib/python3.10/site-packages/pyspark/sql/session.py\", line 931, in _createFromRDD\n",
      "    struct = self._inferSchema(rdd, samplingRatio, names=schema)\n",
      "  File \"/home/ilyes/.local/lib/python3.10/site-packages/pyspark/sql/session.py\", line 874, in _inferSchema\n",
      "    first = rdd.first()\n",
      "  File \"/home/ilyes/.local/lib/python3.10/site-packages/pyspark/rdd.py\", line 2872, in first\n",
      "    raise ValueError(\"RDD is empty\")\n",
      "ValueError: RDD is empty\n",
      "\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.callPythonTransformFunction(PythonDStream.scala:95)\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.apply(PythonDStream.scala:78)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1$adapted(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$2(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.streaming.dstream.DStream.createRDDWithLocalProperties(DStream.scala:417)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$1(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.Try$.apply(Try.scala:213)\n",
      "\tat org.apache.spark.streaming.scheduler.Job.run(Job.scala:39)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.$anonfun$run$1(JobScheduler.scala:256)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.run(JobScheduler.scala:256)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:829)\n",
      "23/08/23 21:04:41 ERROR JobScheduler: Error running job streaming job 1692817480000 ms.0\n",
      "org.apache.spark.SparkException: An exception was raised by Python:\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/ilyes/.local/lib/python3.10/site-packages/pyspark/streaming/util.py\", line 71, in call\n",
      "    r = self.func(t, *rdds)\n",
      "  File \"/home/ilyes/.local/lib/python3.10/site-packages/pyspark/streaming/dstream.py\", line 236, in func\n",
      "    return old_func(rdd)  # type: ignore[call-arg, arg-type]\n",
      "  File \"/tmp/ipykernel_4072/1757346720.py\", line 7, in <lambda>\n",
      "    .foreachRDD( lambda rdd: rdd.toDF().sort( desc(\"count\") ) # Sorts Them in a DF\n",
      "  File \"/home/ilyes/.local/lib/python3.10/site-packages/pyspark/sql/session.py\", line 115, in toDF\n",
      "    return sparkSession.createDataFrame(self, schema, sampleRatio)\n",
      "  File \"/home/ilyes/.local/lib/python3.10/site-packages/pyspark/sql/session.py\", line 1276, in createDataFrame\n",
      "    return self._create_dataframe(\n",
      "  File \"/home/ilyes/.local/lib/python3.10/site-packages/pyspark/sql/session.py\", line 1316, in _create_dataframe\n",
      "    rdd, struct = self._createFromRDD(data.map(prepare), schema, samplingRatio)\n",
      "  File \"/home/ilyes/.local/lib/python3.10/site-packages/pyspark/sql/session.py\", line 931, in _createFromRDD\n",
      "    struct = self._inferSchema(rdd, samplingRatio, names=schema)\n",
      "  File \"/home/ilyes/.local/lib/python3.10/site-packages/pyspark/sql/session.py\", line 874, in _inferSchema\n",
      "    first = rdd.first()\n",
      "  File \"/home/ilyes/.local/lib/python3.10/site-packages/pyspark/rdd.py\", line 2872, in first\n",
      "    raise ValueError(\"RDD is empty\")\n",
      "ValueError: RDD is empty\n",
      "\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.callPythonTransformFunction(PythonDStream.scala:95)\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.apply(PythonDStream.scala:78)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1$adapted(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$2(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.streaming.dstream.DStream.createRDDWithLocalProperties(DStream.scala:417)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$1(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.Try$.apply(Try.scala:213)\n",
      "\tat org.apache.spark.streaming.scheduler.Job.run(Job.scala:39)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.$anonfun$run$1(JobScheduler.scala:256)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.run(JobScheduler.scala:256)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:829)\n",
      "23/08/23 21:04:46 WARN ReceiverSupervisorImpl: Restarting receiver with delay 2000 ms: Socket data stream had no more data\n",
      "23/08/23 21:04:46 ERROR ReceiverTracker: Deregistered receiver for stream 0: Restarting receiver with delay 2000ms: Socket data stream had no more data\n",
      "23/08/23 21:04:48 WARN ReceiverSupervisorImpl: Restarting receiver with delay 2000 ms: Error connecting to 127.0.0.1:9998\n",
      "java.net.ConnectException: Connection refused (Connection refused)\n",
      "\tat java.base/java.net.PlainSocketImpl.socketConnect(Native Method)\n",
      "\tat java.base/java.net.AbstractPlainSocketImpl.doConnect(AbstractPlainSocketImpl.java:412)\n",
      "\tat java.base/java.net.AbstractPlainSocketImpl.connectToAddress(AbstractPlainSocketImpl.java:255)\n",
      "\tat java.base/java.net.AbstractPlainSocketImpl.connect(AbstractPlainSocketImpl.java:237)\n",
      "\tat java.base/java.net.SocksSocketImpl.connect(SocksSocketImpl.java:392)\n",
      "\tat java.base/java.net.Socket.connect(Socket.java:609)\n",
      "\tat java.base/java.net.Socket.connect(Socket.java:558)\n",
      "\tat java.base/java.net.Socket.<init>(Socket.java:454)\n",
      "\tat java.base/java.net.Socket.<init>(Socket.java:231)\n",
      "\tat org.apache.spark.streaming.dstream.SocketReceiver.onStart(SocketInputDStream.scala:61)\n",
      "\tat org.apache.spark.streaming.receiver.ReceiverSupervisor.startReceiver(ReceiverSupervisor.scala:149)\n",
      "\tat org.apache.spark.streaming.receiver.ReceiverSupervisor.$anonfun$restartReceiver$1(ReceiverSupervisor.scala:198)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.concurrent.Future$.$anonfun$apply$1(Future.scala:659)\n",
      "\tat scala.util.Success.$anonfun$map$1(Try.scala:255)\n",
      "\tat scala.util.Success.map(Try.scala:213)\n",
      "\tat scala.concurrent.Future.$anonfun$map$1(Future.scala:292)\n",
      "\tat scala.concurrent.impl.Promise.liftedTree1$1(Promise.scala:33)\n",
      "\tat scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:829)\n",
      "23/08/23 21:04:48 ERROR ReceiverTracker: Deregistered receiver for stream 0: Restarting receiver with delay 2000ms: Error connecting to 127.0.0.1:9998 - java.net.ConnectException: Connection refused (Connection refused)\n",
      "\tat java.base/java.net.PlainSocketImpl.socketConnect(Native Method)\n",
      "\tat java.base/java.net.AbstractPlainSocketImpl.doConnect(AbstractPlainSocketImpl.java:412)\n",
      "\tat java.base/java.net.AbstractPlainSocketImpl.connectToAddress(AbstractPlainSocketImpl.java:255)\n",
      "\tat java.base/java.net.AbstractPlainSocketImpl.connect(AbstractPlainSocketImpl.java:237)\n",
      "\tat java.base/java.net.SocksSocketImpl.connect(SocksSocketImpl.java:392)\n",
      "\tat java.base/java.net.Socket.connect(Socket.java:609)\n",
      "\tat java.base/java.net.Socket.connect(Socket.java:558)\n",
      "\tat java.base/java.net.Socket.<init>(Socket.java:454)\n",
      "\tat java.base/java.net.Socket.<init>(Socket.java:231)\n",
      "\tat org.apache.spark.streaming.dstream.SocketReceiver.onStart(SocketInputDStream.scala:61)\n",
      "\tat org.apache.spark.streaming.receiver.ReceiverSupervisor.startReceiver(ReceiverSupervisor.scala:149)\n",
      "\tat org.apache.spark.streaming.receiver.ReceiverSupervisor.$anonfun$restartReceiver$1(ReceiverSupervisor.scala:198)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.concurrent.Future$.$anonfun$apply$1(Future.scala:659)\n",
      "\tat scala.util.Success.$anonfun$map$1(Try.scala:255)\n",
      "\tat scala.util.Success.map(Try.scala:213)\n",
      "\tat scala.concurrent.Future.$anonfun$map$1(Future.scala:292)\n",
      "\tat scala.concurrent.impl.Promise.liftedTree1$1(Promise.scala:33)\n",
      "\tat scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:829)\n",
      "\n",
      "23/08/23 21:04:50 WARN ReceiverSupervisorImpl: Restarting receiver with delay 2000 ms: Error connecting to 127.0.0.1:9998\n",
      "java.net.ConnectException: Connection refused (Connection refused)\n",
      "\tat java.base/java.net.PlainSocketImpl.socketConnect(Native Method)\n",
      "\tat java.base/java.net.AbstractPlainSocketImpl.doConnect(AbstractPlainSocketImpl.java:412)\n",
      "\tat java.base/java.net.AbstractPlainSocketImpl.connectToAddress(AbstractPlainSocketImpl.java:255)\n",
      "\tat java.base/java.net.AbstractPlainSocketImpl.connect(AbstractPlainSocketImpl.java:237)\n",
      "\tat java.base/java.net.SocksSocketImpl.connect(SocksSocketImpl.java:392)\n",
      "\tat java.base/java.net.Socket.connect(Socket.java:609)\n",
      "\tat java.base/java.net.Socket.connect(Socket.java:558)\n",
      "\tat java.base/java.net.Socket.<init>(Socket.java:454)\n",
      "\tat java.base/java.net.Socket.<init>(Socket.java:231)\n",
      "\tat org.apache.spark.streaming.dstream.SocketReceiver.onStart(SocketInputDStream.scala:61)\n",
      "\tat org.apache.spark.streaming.receiver.ReceiverSupervisor.startReceiver(ReceiverSupervisor.scala:149)\n",
      "\tat org.apache.spark.streaming.receiver.ReceiverSupervisor.$anonfun$restartReceiver$1(ReceiverSupervisor.scala:198)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.concurrent.Future$.$anonfun$apply$1(Future.scala:659)\n",
      "\tat scala.util.Success.$anonfun$map$1(Try.scala:255)\n",
      "\tat scala.util.Success.map(Try.scala:213)\n",
      "\tat scala.concurrent.Future.$anonfun$map$1(Future.scala:292)\n",
      "\tat scala.concurrent.impl.Promise.liftedTree1$1(Promise.scala:33)\n",
      "\tat scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:829)\n",
      "23/08/23 21:04:50 ERROR ReceiverTracker: Deregistered receiver for stream 0: Restarting receiver with delay 2000ms: Error connecting to 127.0.0.1:9998 - java.net.ConnectException: Connection refused (Connection refused)\n",
      "\tat java.base/java.net.PlainSocketImpl.socketConnect(Native Method)\n",
      "\tat java.base/java.net.AbstractPlainSocketImpl.doConnect(AbstractPlainSocketImpl.java:412)\n",
      "\tat java.base/java.net.AbstractPlainSocketImpl.connectToAddress(AbstractPlainSocketImpl.java:255)\n",
      "\tat java.base/java.net.AbstractPlainSocketImpl.connect(AbstractPlainSocketImpl.java:237)\n",
      "\tat java.base/java.net.SocksSocketImpl.connect(SocksSocketImpl.java:392)\n",
      "\tat java.base/java.net.Socket.connect(Socket.java:609)\n",
      "\tat java.base/java.net.Socket.connect(Socket.java:558)\n",
      "\tat java.base/java.net.Socket.<init>(Socket.java:454)\n",
      "\tat java.base/java.net.Socket.<init>(Socket.java:231)\n",
      "\tat org.apache.spark.streaming.dstream.SocketReceiver.onStart(SocketInputDStream.scala:61)\n",
      "\tat org.apache.spark.streaming.receiver.ReceiverSupervisor.startReceiver(ReceiverSupervisor.scala:149)\n",
      "\tat org.apache.spark.streaming.receiver.ReceiverSupervisor.$anonfun$restartReceiver$1(ReceiverSupervisor.scala:198)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.concurrent.Future$.$anonfun$apply$1(Future.scala:659)\n",
      "\tat scala.util.Success.$anonfun$map$1(Try.scala:255)\n",
      "\tat scala.util.Success.map(Try.scala:213)\n",
      "\tat scala.concurrent.Future.$anonfun$map$1(Future.scala:292)\n",
      "\tat scala.concurrent.impl.Promise.liftedTree1$1(Promise.scala:33)\n",
      "\tat scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:829)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "ssc.start()    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "23/08/23 20:58:31 ERROR JobScheduler: Error running job streaming job 1692817110000 ms.0\n",
      "org.apache.spark.SparkException: An exception was raised by Python:\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/ilyes/.local/lib/python3.10/site-packages/pyspark/streaming/util.py\", line 71, in call\n",
      "    r = self.func(t, *rdds)\n",
      "  File \"/home/ilyes/.local/lib/python3.10/site-packages/pyspark/streaming/dstream.py\", line 236, in func\n",
      "    return old_func(rdd)  # type: ignore[call-arg, arg-type]\n",
      "  File \"/tmp/ipykernel_7185/1757346720.py\", line 7, in <lambda>\n",
      "    .foreachRDD( lambda rdd: rdd.toDF().sort( desc(\"count\") ) # Sorts Them in a DF\n",
      "  File \"/home/ilyes/.local/lib/python3.10/site-packages/pyspark/sql/session.py\", line 115, in toDF\n",
      "    return sparkSession.createDataFrame(self, schema, sampleRatio)\n",
      "  File \"/home/ilyes/.local/lib/python3.10/site-packages/pyspark/sql/session.py\", line 1276, in createDataFrame\n",
      "    return self._create_dataframe(\n",
      "  File \"/home/ilyes/.local/lib/python3.10/site-packages/pyspark/sql/session.py\", line 1316, in _create_dataframe\n",
      "    rdd, struct = self._createFromRDD(data.map(prepare), schema, samplingRatio)\n",
      "  File \"/home/ilyes/.local/lib/python3.10/site-packages/pyspark/sql/session.py\", line 931, in _createFromRDD\n",
      "    struct = self._inferSchema(rdd, samplingRatio, names=schema)\n",
      "  File \"/home/ilyes/.local/lib/python3.10/site-packages/pyspark/sql/session.py\", line 874, in _inferSchema\n",
      "    first = rdd.first()\n",
      "  File \"/home/ilyes/.local/lib/python3.10/site-packages/pyspark/rdd.py\", line 2872, in first\n",
      "    raise ValueError(\"RDD is empty\")\n",
      "ValueError: RDD is empty\n",
      "\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.callPythonTransformFunction(PythonDStream.scala:95)\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.apply(PythonDStream.scala:78)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1$adapted(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$2(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.streaming.dstream.DStream.createRDDWithLocalProperties(DStream.scala:417)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$1(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.Try$.apply(Try.scala:213)\n",
      "\tat org.apache.spark.streaming.scheduler.Job.run(Job.scala:39)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.$anonfun$run$1(JobScheduler.scala:256)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.run(JobScheduler.scala:256)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:829)\n",
      "23/08/23 20:58:41 ERROR JobScheduler: Error running job streaming job 1692817120000 ms.0\n",
      "org.apache.spark.SparkException: An exception was raised by Python:\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/ilyes/.local/lib/python3.10/site-packages/pyspark/streaming/util.py\", line 71, in call\n",
      "    r = self.func(t, *rdds)\n",
      "  File \"/home/ilyes/.local/lib/python3.10/site-packages/pyspark/streaming/dstream.py\", line 236, in func\n",
      "    return old_func(rdd)  # type: ignore[call-arg, arg-type]\n",
      "  File \"/tmp/ipykernel_7185/1757346720.py\", line 7, in <lambda>\n",
      "    .foreachRDD( lambda rdd: rdd.toDF().sort( desc(\"count\") ) # Sorts Them in a DF\n",
      "  File \"/home/ilyes/.local/lib/python3.10/site-packages/pyspark/sql/session.py\", line 115, in toDF\n",
      "    return sparkSession.createDataFrame(self, schema, sampleRatio)\n",
      "  File \"/home/ilyes/.local/lib/python3.10/site-packages/pyspark/sql/session.py\", line 1276, in createDataFrame\n",
      "    return self._create_dataframe(\n",
      "  File \"/home/ilyes/.local/lib/python3.10/site-packages/pyspark/sql/session.py\", line 1316, in _create_dataframe\n",
      "    rdd, struct = self._createFromRDD(data.map(prepare), schema, samplingRatio)\n",
      "  File \"/home/ilyes/.local/lib/python3.10/site-packages/pyspark/sql/session.py\", line 931, in _createFromRDD\n",
      "    struct = self._inferSchema(rdd, samplingRatio, names=schema)\n",
      "  File \"/home/ilyes/.local/lib/python3.10/site-packages/pyspark/sql/session.py\", line 874, in _inferSchema\n",
      "    first = rdd.first()\n",
      "  File \"/home/ilyes/.local/lib/python3.10/site-packages/pyspark/rdd.py\", line 2872, in first\n",
      "    raise ValueError(\"RDD is empty\")\n",
      "ValueError: RDD is empty\n",
      "\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.callPythonTransformFunction(PythonDStream.scala:95)\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.apply(PythonDStream.scala:78)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1$adapted(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$2(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.streaming.dstream.DStream.createRDDWithLocalProperties(DStream.scala:417)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$1(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.Try$.apply(Try.scala:213)\n",
      "\tat org.apache.spark.streaming.scheduler.Job.run(Job.scala:39)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.$anonfun$run$1(JobScheduler.scala:256)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.run(JobScheduler.scala:256)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:829)\n",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "import time\n",
    "from IPython import display\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "# Only works for Jupyter Notebooks!\n",
    "%matplotlib inline "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "23/08/23 20:58:51 ERROR JobScheduler: Error running job streaming job 1692817130000 ms.0\n",
      "org.apache.spark.SparkException: An exception was raised by Python:\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/ilyes/.local/lib/python3.10/site-packages/pyspark/streaming/util.py\", line 71, in call\n",
      "    r = self.func(t, *rdds)\n",
      "  File \"/home/ilyes/.local/lib/python3.10/site-packages/pyspark/streaming/dstream.py\", line 236, in func\n",
      "    return old_func(rdd)  # type: ignore[call-arg, arg-type]\n",
      "  File \"/tmp/ipykernel_7185/1757346720.py\", line 7, in <lambda>\n",
      "    .foreachRDD( lambda rdd: rdd.toDF().sort( desc(\"count\") ) # Sorts Them in a DF\n",
      "  File \"/home/ilyes/.local/lib/python3.10/site-packages/pyspark/sql/session.py\", line 115, in toDF\n",
      "    return sparkSession.createDataFrame(self, schema, sampleRatio)\n",
      "  File \"/home/ilyes/.local/lib/python3.10/site-packages/pyspark/sql/session.py\", line 1276, in createDataFrame\n",
      "    return self._create_dataframe(\n",
      "  File \"/home/ilyes/.local/lib/python3.10/site-packages/pyspark/sql/session.py\", line 1316, in _create_dataframe\n",
      "    rdd, struct = self._createFromRDD(data.map(prepare), schema, samplingRatio)\n",
      "  File \"/home/ilyes/.local/lib/python3.10/site-packages/pyspark/sql/session.py\", line 931, in _createFromRDD\n",
      "    struct = self._inferSchema(rdd, samplingRatio, names=schema)\n",
      "  File \"/home/ilyes/.local/lib/python3.10/site-packages/pyspark/sql/session.py\", line 874, in _inferSchema\n",
      "    first = rdd.first()\n",
      "  File \"/home/ilyes/.local/lib/python3.10/site-packages/pyspark/rdd.py\", line 2872, in first\n",
      "    raise ValueError(\"RDD is empty\")\n",
      "ValueError: RDD is empty\n",
      "\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.callPythonTransformFunction(PythonDStream.scala:95)\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.apply(PythonDStream.scala:78)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1$adapted(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$2(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.streaming.dstream.DStream.createRDDWithLocalProperties(DStream.scala:417)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$1(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.Try$.apply(Try.scala:213)\n",
      "\tat org.apache.spark.streaming.scheduler.Job.run(Job.scala:39)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.$anonfun$run$1(JobScheduler.scala:256)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.run(JobScheduler.scala:256)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:829)\n",
      "[Stage 0:>                                                          (0 + 1) / 1]\r"
     ]
    },
    {
     "ename": "AnalysisException",
     "evalue": "[TABLE_OR_VIEW_NOT_FOUND] The table or view `tweets` cannot be found. Verify the spelling and correctness of the schema and catalog.\nIf you did not qualify the name with a schema, verify the current_schema() output, or qualify the name with the correct schema and catalog.\nTo tolerate the error on drop use DROP VIEW IF EXISTS or DROP TABLE IF EXISTS.; line 1 pos 23;\n'Project ['tag, 'count]\n+- 'UnresolvedRelation [tweets], [], false\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAnalysisException\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[10], line 5\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[39mwhile\u001b[39;00m count \u001b[39m<\u001b[39m \u001b[39m10\u001b[39m:\n\u001b[1;32m      4\u001b[0m     time\u001b[39m.\u001b[39msleep( \u001b[39m3\u001b[39m )\n\u001b[0;32m----> 5\u001b[0m     top_10_tweets \u001b[39m=\u001b[39m sqlContext\u001b[39m.\u001b[39;49msql( \u001b[39m'\u001b[39;49m\u001b[39mSelect tag, count from tweets\u001b[39;49m\u001b[39m'\u001b[39;49m )\n\u001b[1;32m      6\u001b[0m     top_10_df \u001b[39m=\u001b[39m top_10_tweets\u001b[39m.\u001b[39mtoPandas()\n\u001b[1;32m      7\u001b[0m     display\u001b[39m.\u001b[39mclear_output(wait\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m)\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/pyspark/sql/context.py:547\u001b[0m, in \u001b[0;36mSQLContext.sql\u001b[0;34m(self, sqlQuery)\u001b[0m\n\u001b[1;32m    531\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39msql\u001b[39m(\u001b[39mself\u001b[39m, sqlQuery: \u001b[39mstr\u001b[39m) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m DataFrame:\n\u001b[1;32m    532\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"Returns a :class:`DataFrame` representing the result of the given query.\u001b[39;00m\n\u001b[1;32m    533\u001b[0m \n\u001b[1;32m    534\u001b[0m \u001b[39m    .. versionadded:: 1.0.0\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    545\u001b[0m \u001b[39m    [Row(f1=1, f2='row1'), Row(f1=2, f2='row2'), Row(f1=3, f2='row3')]\u001b[39;00m\n\u001b[1;32m    546\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 547\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49msparkSession\u001b[39m.\u001b[39;49msql(sqlQuery)\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/pyspark/sql/session.py:1440\u001b[0m, in \u001b[0;36mSparkSession.sql\u001b[0;34m(self, sqlQuery, args, **kwargs)\u001b[0m\n\u001b[1;32m   1438\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m   1439\u001b[0m     litArgs \u001b[39m=\u001b[39m {k: _to_java_column(lit(v)) \u001b[39mfor\u001b[39;00m k, v \u001b[39min\u001b[39;00m (args \u001b[39mor\u001b[39;00m {})\u001b[39m.\u001b[39mitems()}\n\u001b[0;32m-> 1440\u001b[0m     \u001b[39mreturn\u001b[39;00m DataFrame(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_jsparkSession\u001b[39m.\u001b[39;49msql(sqlQuery, litArgs), \u001b[39mself\u001b[39m)\n\u001b[1;32m   1441\u001b[0m \u001b[39mfinally\u001b[39;00m:\n\u001b[1;32m   1442\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mlen\u001b[39m(kwargs) \u001b[39m>\u001b[39m \u001b[39m0\u001b[39m:\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/py4j/java_gateway.py:1322\u001b[0m, in \u001b[0;36mJavaMember.__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1316\u001b[0m command \u001b[39m=\u001b[39m proto\u001b[39m.\u001b[39mCALL_COMMAND_NAME \u001b[39m+\u001b[39m\\\n\u001b[1;32m   1317\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcommand_header \u001b[39m+\u001b[39m\\\n\u001b[1;32m   1318\u001b[0m     args_command \u001b[39m+\u001b[39m\\\n\u001b[1;32m   1319\u001b[0m     proto\u001b[39m.\u001b[39mEND_COMMAND_PART\n\u001b[1;32m   1321\u001b[0m answer \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mgateway_client\u001b[39m.\u001b[39msend_command(command)\n\u001b[0;32m-> 1322\u001b[0m return_value \u001b[39m=\u001b[39m get_return_value(\n\u001b[1;32m   1323\u001b[0m     answer, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mgateway_client, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtarget_id, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mname)\n\u001b[1;32m   1325\u001b[0m \u001b[39mfor\u001b[39;00m temp_arg \u001b[39min\u001b[39;00m temp_args:\n\u001b[1;32m   1326\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mhasattr\u001b[39m(temp_arg, \u001b[39m\"\u001b[39m\u001b[39m_detach\u001b[39m\u001b[39m\"\u001b[39m):\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/pyspark/errors/exceptions/captured.py:175\u001b[0m, in \u001b[0;36mcapture_sql_exception.<locals>.deco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m    171\u001b[0m converted \u001b[39m=\u001b[39m convert_exception(e\u001b[39m.\u001b[39mjava_exception)\n\u001b[1;32m    172\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39misinstance\u001b[39m(converted, UnknownException):\n\u001b[1;32m    173\u001b[0m     \u001b[39m# Hide where the exception came from that shows a non-Pythonic\u001b[39;00m\n\u001b[1;32m    174\u001b[0m     \u001b[39m# JVM exception message.\u001b[39;00m\n\u001b[0;32m--> 175\u001b[0m     \u001b[39mraise\u001b[39;00m converted \u001b[39mfrom\u001b[39;00m \u001b[39mNone\u001b[39;00m\n\u001b[1;32m    176\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    177\u001b[0m     \u001b[39mraise\u001b[39;00m\n",
      "\u001b[0;31mAnalysisException\u001b[0m: [TABLE_OR_VIEW_NOT_FOUND] The table or view `tweets` cannot be found. Verify the spelling and correctness of the schema and catalog.\nIf you did not qualify the name with a schema, verify the current_schema() output, or qualify the name with the correct schema and catalog.\nTo tolerate the error on drop use DROP VIEW IF EXISTS or DROP TABLE IF EXISTS.; line 1 pos 23;\n'Project ['tag, 'count]\n+- 'UnresolvedRelation [tweets], [], false\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "23/08/23 20:59:01 ERROR JobScheduler: Error running job streaming job 1692817140000 ms.0\n",
      "org.apache.spark.SparkException: An exception was raised by Python:\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/ilyes/.local/lib/python3.10/site-packages/pyspark/streaming/util.py\", line 71, in call\n",
      "    r = self.func(t, *rdds)\n",
      "  File \"/home/ilyes/.local/lib/python3.10/site-packages/pyspark/streaming/dstream.py\", line 236, in func\n",
      "    return old_func(rdd)  # type: ignore[call-arg, arg-type]\n",
      "  File \"/tmp/ipykernel_7185/1757346720.py\", line 7, in <lambda>\n",
      "    .foreachRDD( lambda rdd: rdd.toDF().sort( desc(\"count\") ) # Sorts Them in a DF\n",
      "  File \"/home/ilyes/.local/lib/python3.10/site-packages/pyspark/sql/session.py\", line 115, in toDF\n",
      "    return sparkSession.createDataFrame(self, schema, sampleRatio)\n",
      "  File \"/home/ilyes/.local/lib/python3.10/site-packages/pyspark/sql/session.py\", line 1276, in createDataFrame\n",
      "    return self._create_dataframe(\n",
      "  File \"/home/ilyes/.local/lib/python3.10/site-packages/pyspark/sql/session.py\", line 1316, in _create_dataframe\n",
      "    rdd, struct = self._createFromRDD(data.map(prepare), schema, samplingRatio)\n",
      "  File \"/home/ilyes/.local/lib/python3.10/site-packages/pyspark/sql/session.py\", line 931, in _createFromRDD\n",
      "    struct = self._inferSchema(rdd, samplingRatio, names=schema)\n",
      "  File \"/home/ilyes/.local/lib/python3.10/site-packages/pyspark/sql/session.py\", line 874, in _inferSchema\n",
      "    first = rdd.first()\n",
      "  File \"/home/ilyes/.local/lib/python3.10/site-packages/pyspark/rdd.py\", line 2872, in first\n",
      "    raise ValueError(\"RDD is empty\")\n",
      "ValueError: RDD is empty\n",
      "\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.callPythonTransformFunction(PythonDStream.scala:95)\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.apply(PythonDStream.scala:78)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1$adapted(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$2(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.streaming.dstream.DStream.createRDDWithLocalProperties(DStream.scala:417)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$1(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.Try$.apply(Try.scala:213)\n",
      "\tat org.apache.spark.streaming.scheduler.Job.run(Job.scala:39)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.$anonfun$run$1(JobScheduler.scala:256)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.run(JobScheduler.scala:256)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:829)\n",
      "[Stage 0:>                                                          (0 + 1) / 1]\r"
     ]
    }
   ],
   "source": [
    "count = 0\n",
    "while count < 10:\n",
    "    \n",
    "    time.sleep( 3 )\n",
    "    top_10_tweets = sqlContext.sql( 'Select tag, count from tweets' )\n",
    "    top_10_df = top_10_tweets.toPandas()\n",
    "    display.clear_output(wait=True)\n",
    "    sns.plt.figure( figsize = ( 10, 8 ) )\n",
    "    sns.barplot( x=\"count\", y=\"tag\", data=top_10_df)\n",
    "    sns.plt.show()\n",
    "    count = count + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "23/08/23 21:04:50 WARN ReceiverSupervisorImpl: Receiver has been stopped\n",
      "Exception in thread \"receiver-supervisor-future-2\" java.lang.InterruptedException: sleep interrupted\n",
      "\tat java.base/java.lang.Thread.sleep(Native Method)\n",
      "\tat org.apache.spark.streaming.receiver.ReceiverSupervisor.$anonfun$restartReceiver$1(ReceiverSupervisor.scala:196)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.concurrent.Future$.$anonfun$apply$1(Future.scala:659)\n",
      "\tat scala.util.Success.$anonfun$map$1(Try.scala:255)\n",
      "\tat scala.util.Success.map(Try.scala:213)\n",
      "\tat scala.concurrent.Future.$anonfun$map$1(Future.scala:292)\n",
      "\tat scala.concurrent.impl.Promise.liftedTree1$1(Promise.scala:33)\n",
      "\tat scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:829)\n",
      "23/08/23 21:04:51 ERROR JobScheduler: Error running job streaming job 1692817490000 ms.0\n",
      "org.apache.spark.SparkException: An exception was raised by Python:\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/ilyes/.local/lib/python3.10/site-packages/pyspark/streaming/util.py\", line 71, in call\n",
      "    r = self.func(t, *rdds)\n",
      "  File \"/home/ilyes/.local/lib/python3.10/site-packages/pyspark/streaming/dstream.py\", line 236, in func\n",
      "    return old_func(rdd)  # type: ignore[call-arg, arg-type]\n",
      "  File \"/tmp/ipykernel_4072/1757346720.py\", line 7, in <lambda>\n",
      "    .foreachRDD( lambda rdd: rdd.toDF().sort( desc(\"count\") ) # Sorts Them in a DF\n",
      "  File \"/home/ilyes/.local/lib/python3.10/site-packages/pyspark/sql/session.py\", line 115, in toDF\n",
      "    return sparkSession.createDataFrame(self, schema, sampleRatio)\n",
      "  File \"/home/ilyes/.local/lib/python3.10/site-packages/pyspark/sql/session.py\", line 1276, in createDataFrame\n",
      "    return self._create_dataframe(\n",
      "  File \"/home/ilyes/.local/lib/python3.10/site-packages/pyspark/sql/session.py\", line 1316, in _create_dataframe\n",
      "    rdd, struct = self._createFromRDD(data.map(prepare), schema, samplingRatio)\n",
      "  File \"/home/ilyes/.local/lib/python3.10/site-packages/pyspark/sql/session.py\", line 931, in _createFromRDD\n",
      "    struct = self._inferSchema(rdd, samplingRatio, names=schema)\n",
      "  File \"/home/ilyes/.local/lib/python3.10/site-packages/pyspark/sql/session.py\", line 874, in _inferSchema\n",
      "    first = rdd.first()\n",
      "  File \"/home/ilyes/.local/lib/python3.10/site-packages/pyspark/rdd.py\", line 2872, in first\n",
      "    raise ValueError(\"RDD is empty\")\n",
      "ValueError: RDD is empty\n",
      "\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.callPythonTransformFunction(PythonDStream.scala:95)\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.apply(PythonDStream.scala:78)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1$adapted(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$2(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.streaming.dstream.DStream.createRDDWithLocalProperties(DStream.scala:417)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$1(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.Try$.apply(Try.scala:213)\n",
      "\tat org.apache.spark.streaming.scheduler.Job.run(Job.scala:39)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.$anonfun$run$1(JobScheduler.scala:256)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.run(JobScheduler.scala:256)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:829)\n"
     ]
    }
   ],
   "source": [
    "ssc.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
